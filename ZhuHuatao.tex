\documentclass[12pt,a4paper]{article}  % Use this line if this document will be released
%\documentclass[12pt,a4paper,draft]{article}  % Use this line if this document is a draft
\usepackage{ifdraft}


%% Bibliography
\usepackage{etoolbox}
\newcommand{\bibfile}{\jobname.bib}  % Name of the BibTeX file.
% ref.bib should be a symbolic link to the universal BibTeX file, which should be a local copy of
% https://github.com/equipez/bibliographie/blob/main/ref.bib
% Run `getbib` in the current directory under the draft mode to get the BibTeX file containing only
% the cited references. The name will be xyz.bib if this TeX file is xyz.tex.
\newcommand{\universalbib}{ref.bib}
\ifdraft{\IfFileExists{\universalbib}{\renewcommand{\bibfile}{\universalbib}}{}}{}
% The counter `cite' is used to count the number of citations.
\newcounter{cite}
\pretocmd{\cite}{\stepcounter{cite}}{}{}


%% Add line numbers in draft mode
\RequirePackage[mathlines]{lineno}
\ifdraft{\linenumbers}{}
\renewcommand{\linenumberfont}{\normalfont\scriptsize\sffamily\color{gray}}
\setlength{\linenumbersep}{\marginparsep}


%% Geometry
%\voffset=-1.5cm \hoffset=-1.4cm \textwidth=16cm \textheight=22.0cm  % Luis' setting
\usepackage[a4paper, textwidth=16.0cm, textheight=22.0cm]{geometry}
\renewcommand{\baselinestretch}{1.2}


%% Basic packages
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{mathtools}  % Provides \coloneqq
\usepackage{empheq}
\usepackage{xcolor}
\usepackage[bbgreekl]{mathbbol}
\DeclareSymbolFontAlphabet{\mathbbm}{bbold}
\DeclareSymbolFontAlphabet{\mathbb}{AMSb}
\usepackage{bbm}
\usepackage{upgreek}
\usepackage{accents}
\usepackage{xspace}
\usepackage{rotating}
\usepackage{multirow,booktabs}
\usepackage[en-US]{datetime2}


%% Format of the table of content
\usepackage[normalem]{ulem}
\usepackage[toc,page]{appendix}
\renewcommand{\appendixpagename}{\Large{Appendix}}
\renewcommand{\appendixname}{Appendix}
\renewcommand{\appendixtocname}{Appendix}
%\usepackage{sectsty}
\setcounter{tocdepth}{2}


%% Section title style
\usepackage{sectsty}
\sectionfont{\large}
\subsectionfont{\large}


%% Some colors
\definecolor{darkblue}{rgb}{0,0.1,0.5}
\definecolor{darkgreen}{rgb}{0,0.5,0.1}
\definecolor{darkyellow}{rgb}{0.65,0.65,0.01}


%% Todo notes
\ifdraft{
    \setlength{\marginparwidth}{2.42cm}
    \usepackage[tickmarkheight=3pt,textsize=small,backgroundcolor=blue!16,linecolor=purple,bordercolor=purple]{todonotes}
}{
    \newcommand{\todo}[1]{}
    \newcommand{\listoftodos}{}
}


%% Graph, tikz and pgf
%\usepackage{subfigure}
\setlength{\unitlength}{1mm}
% The \unitlength command is a Length command. It defines the units used in the Picture Environment.
\usepackage{graphicx}
%\usepackage{tikz,tikzscale,pgf,pgfarrows,pgfnodes,filecontents,tikz-cd}
\usepackage{tikz,tikzscale,pgf}
\usetikzlibrary{arrows,arrows.meta,patterns,positioning,decorations.markings,shapes}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage[justification=centering]{caption}
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.11}


%% Turn off some unharmful warnings in draft mode
%% N.B.: DO NOT use `silence` together with `hyperref`. They will cause an infinite loop.
\ifdraft{
    \usepackage{silence}
    \WarningFilter{xcolor}{Incompatible color definition on}
    \WarningFilter{hyperref}{Draft mode on}
    \WarningFilter{refcheck}{Unused label}
    \WarningFilter{microtype}{`draft' option active}
    \WarningFilter{latex}{Writing or overwriting file} % Mute the warning about 'writing/overwriting file'
    \WarningFilter{latex}{Writing file} % Mute the warning about 'writing/overwriting file'
    \WarningFilter{latex}{Tab has} % Mute the warning about 'Tab has been converted to Blank Space'
    \WarningFilter{latex}{Marginpar on page} % Mute the warning about 'Marginpar on page xx moved'
    \WarningFilter{latex}{author given} % Mute the warning about 'No \author given'
}{}


%% Hyperref, url, and email
%% N.B.: DO NOT use `silence` together with `hyperref`. They will cause an infinite loop.
\ifdraft{\usepackage{refcheck}\newcommand{\url}{\texttt}}{
    \usepackage{hyperref}
    \hypersetup{colorlinks, linkcolor=darkblue, anchorcolor=darkblue, citecolor=darkblue, urlcolor=darkblue}
    \usepackage{url}
} % Check unused labels
\newcommand{\email}{\texttt}


%% Enumerate and itemize
\usepackage{eqlist}
\usepackage{enumitem}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*,label=\normalfont{(\alph*)}}


%% Algorithm environment
\usepackage[section]{algorithm}
\usepackage{algpseudocode,algorithmicx}
\newcommand{\INPUT}{\textbf{Input}}
\newcommand{\FOR}{\textbf{For}~}
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}
\algrenewcommand\alglinenumber[1]{\normalsize #1.}
\newcommand*\Let[2]{\State #1 $=$ #2}


%% Theorem-like environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{exercise}{Exercise}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{problem}{Problem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{example}{Example}[section]
\newtheorem{question}{Question}[section]
% Change theoremstyle to ``definition'', which uses textnormal for the text.
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[section]
% proof
\usepackage{xpatch}
\xpatchcmd{\proof}{\itshape}{\normalfont\proofnamefont}{}{}
\newcommand{\proofnamefont}{\bfseries}

%% Equation numbering
\numberwithin{equation}{section}


%% Fine tuning
\usepackage{microtype}
\usepackage[nobottomtitles*]{titlesec} % No section title at the bottom of pages
% Prevent footnote from running to the next page
\interfootnotelinepenalty=10000
% No line break in inline math
\interdisplaylinepenalty=10000
\relpenalty=10000
\binoppenalty=10000
% No widow or orphan lines
\clubpenalty=10000
\widowpenalty=10000
\displaywidowpenalty=10000


% Use @ to put 1 math unit (mu) in math
% See https://nhigham.com/2013/01/07/fine-tuning-spacing-in-latex-equations/
% and also TeXbook p. 155.
\mathcode`@="8000{\catcode`\@=\active\gdef@{\mkern1mu}}


%% Operators, commands
\usepackage{relsize}
\usepackage{nccmath}
%\DeclareMathOperator*{\mcap}{\,\medmath{\bigcap}\,}
%\DeclareMathOperator*{\mcup}{\,\medmath{\bigcup}\,}
\DeclareMathOperator*{\mcap}{\,\mathsmaller{\bigcap}\,}
\DeclareMathOperator*{\mcup}{\,\mathsmaller{\bigcup}\,}
%\renewcommand{\cap}{\mcap}
%\renewcommand{\cup}{\mcup}

\newcommand{\ceil}[1]{ {\lceil{#1}\rceil} }
\newcommand{\floor}[1]{ {\lfloor{#1}\rfloor} }

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\sort}{sort}
\DeclareMathOperator*{\Argmax}{Argmax}
\DeclareMathOperator*{\Argmin}{Argmin}
\DeclareMathOperator*{\Arglocmin}{Arglocmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\Diag}{Diag}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\med}{med}
\DeclareMathOperator{\essinf}{essinf}
\DeclareMathOperator{\cl}{cl}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\comp}{C}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\card}{card}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator{\dist}{dist}
\newcommand{\disth}{{\operatorname{\updelta_{\sss{H}}}}}
\newcommand{\ind}{\mathbbm{1}}
%\newcommand*{\defeq}{\stackrel{\mbox{\normalfont\tiny{\textnormal{def}}}}{=}}
\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\BB}{\mathcal{B}}
\renewcommand{\SS}{\mathbb{S}}
\newcommand{\TT}{\mathcal{T}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\XX}{\mathcal{X}}
\newcommand{\sset}{\mathcal{S}}
\newcommand{\pen}{h}
\newcommand{\penpar}{\mu}
\newcommand{\res}{\rho}
\newcommand{\col}{r}
\newcommand{\ofd}{\mathcal{F}}
\newcommand{\stf}[1]{\mathbb{S}^{#1}}
\newcommand{\sss}[1]{{\scriptscriptstyle{#1}}}
\newcommand{\sK}{{\scriptscriptstyle{K}}}
\newcommand{\sT}{{\scriptscriptstyle{T}}}
\newcommand{\fro}{{\scriptstyle{\textnormal{F}}}}
\newcommand{\trs}{{\scriptstyle{\mathsf{T}}}}
\newcommand{\hmt}{{\scriptstyle{{\mathsf{H}}}}}
\newcommand{\pin}{{\scriptstyle{{\mathsf{+}}}}}
\newcommand{\inv}{{-1}}
\newcommand{\adj}{*}
\newcommand{\ones}{\mathbf{1}}

\newcommand{\cs}{\text{c}}
\newcommand{\hp}{\circ}
\newcommand{\cc}{\sss{\textnormal{C}}}
\newcommand{\dec}{\sss{\textnormal{D}}}
\newcommand{\cauchy}{\sss{\textnormal{C}}}
\newcommand{\scauchy}{\sss{\textnormal{S}}}
\newcommand{\crit}{\textnormal{crit}}
\newcommand{\rsg}{\hat{\partial}}
\newcommand{\gsg}{\partial}
\newcommand{\dom}{\textnormal{dom}}
\newcommand{\tf}{{\textnormal{f}}}
\newcommand{\tg}{{\textnormal{g}}}
\newcommand{\ts}{{\textnormal{s}}}
\newcommand{\st}{\textnormal{s.t.}}
\newcommand{\etc}{{etc.}\xspace}
\newcommand{\ie}{{i.e.}\xspace}
\newcommand{\eg}{{e.g.}\xspace}
\newcommand{\etal}{{et al.}\xspace}
\newcommand{\iid}{\text{i.i.d.}\xspace}
\newcommand{\as}{\text{a.s.}\xspace}

\newcommand{\me}{\mathrm{e}}
\newcommand{\md}{\mathrm{d}}
\newcommand{\mi}{\mathrm{i}}
\newcommand{\lev}{\mathrm{lev}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bx}{\mathbf{u}}
%\newcommand{\bb}{\mathbf{f}}
\newcommand{\bb}{\mathbf{r}}
\newcommand{\nov}{n_{\textnormal{o}}}
\xspaceaddexceptions{]\}}
% tex.stackexchange.com/questions/15252/why-does-xspace-behave-differently-for-parenthesis-vs-braces-brackets
\newcommand{\MATLAB}{\textsc{Matlab}\xspace}
\newcommand{\octave}{\mbox{GNU Octave}\xspace}
\newcommand{\prblm}{\texttt}
\DeclareMathAlphabet{\mathsfit}{T1}{\sfdefault}{\mddefault}{\sldefault}
\SetMathAlphabet{\mathsfit}{bold}{T1}{\sfdefault}{\bfdefault}{\sldefault}
\newcommand{\prbb}{\mathsfit{p}}
\newcommand{\pp}{\mathsf{p}}
\newcommand{\qq}{\mathsf{q}}
\newcommand{\ttt}{\mathsfit{t}}
\newcommand{\tol}{\varepsilon}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\dd}{\mathbf{d}}
\newcommand{\ii}{\mathbf{i}}
\newcommand{\jj}{\mathbf{j}}
\newcommand{\xx}{\mathbf{x}}
\renewcommand{\pp}{\mathbf{p}}
\renewcommand{\ggg}{\mathbf{g}}
\newcommand{\GG}{\mathbf{G}}
\DeclareMathOperator{\expc}{\mathbb{E}}
\renewcommand{\Pr}{\mathbb{P}}
\newcommand{\lb}{\underline}
\newcommand{\ub}{\overline}

% mathlcal font
\DeclareFontFamily{U}{dutchcal}{\skewchar\font=45 }
\DeclareFontShape{U}{dutchcal}{m}{n}{<-> s*[1.0] dutchcal-r}{}
\DeclareFontShape{U}{dutchcal}{b}{n}{<-> s*[1.0] dutchcal-b}{}
\DeclareMathAlphabet{\mathlcal}{U}{dutchcal}{m}{n}
\SetMathAlphabet{\mathlcal}{bold}{U}{dutchcal}{b}{n}

% mathscr font (supporting lowercase letters)
%\usepackage[scr=dutchcal]{mathalfa}
%\usepackage[scr=esstix]{mathalfa}
%\usepackage[scr=boondox]{mathalfa}
%\usepackage[scr=boondoxo]{mathalfa}
\usepackage[scr=boondoxupr]{mathalfa}
%\newcommand{\model}{\mathscr{h}}
\newcommand{\model}{\tilde{f}}
\newcommand{\rmod}{F}

\newcommand{\Set}[1]{\mathcal{#1}}
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it} % The mathpzc font
\newcommand{\slv}{\mathpzc}
% mathpzc looks great, but it stops working on 19 Feb 2020 for no reason.
%\newcommand{\slv}{\mathscr}
\newcommand{\software}{\texttt}
\DeclareMathOperator{\eff}{\mathsf{e}\;\!}
\DeclareMathOperator{\Eff}{\mathsf{E}\;\!}
\newcommand{\out}{{\text{out}}}


%% Commands for revision
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\green}[1]{\textcolor{darkgreen}{#1}}
\newcommand{\TYPO}[1]{{\color{orange}{#1}}}
\newcommand{\MISTAKE}[1]{{\color{violet}{#1}}}
\newcommand{\REPHRASE}[1]{{\color{darkgreen}{#1}}}
\newcommand{\REVISE}[1]{{\color{blue}{#1}}}
\newcommand{\REVISEred}[1]{{\color{red}{#1}}}
\newcommand{\COMMENT}{\todo}  % Needs the todonotes package
%\newcommand{\COMMENT}[1]{\textcolor{brown}{{\small{(comment: #1)}}}}  % This puts comments inline

% Use the following if revision is finished
%\newcommand{\TYPO}{}
%\newcommand{\MISTAKE}{}
%\newcommand{\REPHRASE}{}
%\newcommand{\REVISE}{}
%\newcommand{\REVISEred}{}
%\newcommand{\COMMENT}[1]{}  % Input ignored.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{ }

\date{\DTMnow}

\author{
    Zhuhuatao
    \thanks{}
    %\and
    %Author2
    %\thanks{Information2}
}


\begin{document}

\maketitle

%\begin{abstract}
%\end{abstract}

%\textbf{Keywords}: Keyword1, Keyword2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \section{}
        \noindent (a) \\
        By the $L$-Lipschitz continuity of $\nabla f$ we have for all $x,y\in\mathbb{R}^n$:
        \[
            f(y) \le f(x) + \nabla f(x)^\top (y-x) + \frac{L}{2}\|y-x\|_2^2.
        \]
        Define
        \[
            g(y) := f(x) + \nabla f(x)^\top (y-x) + \frac{L}{2}\|y-x\|_2^2.
        \]
        Then
        \[\nabla g(y)=\nabla f(x) + L(y-x),\]
        which has the unique stationary point
        \[y_0 = x - \frac{1}{L}\nabla f(x).
        \]
        Substituting $y_0$ into $g$ gives
        \[
            g(y_0) = f(x) - \frac{1}{2L}\|\nabla f(x)\|_2^2.
        \]
        Since $f(y)\le g(y)$ for all $y$, we obtain
        \[
            \inf_{y\in\mathbb{R}^n} f(y) \le f(y_0) \le f(x) - \frac{1}{2L}\|\nabla f(x)\|_2^2.
        \]

        \noindent (b) \\
        Let
        \[h(y) := f(y) - f(x) - \nabla f(x)^\top (y-x).
        \]
        By convexity of $f$ we have $h(y)\ge 0$ for all $y$ and $h(x)=0$. Moreover
        \[\nabla h(y)=\nabla f(y)-\nabla f(x),\]
        which is $L$-Lipschitz continous. Applying the estimate from part (a) to the convex function
        $h$ (noting that $\inf h=0$) yields
        \[
            h(y) \ge \frac{1}{2L}\|\nabla h(y)\|_2^2.
        \]
        Substituting back $h$ and its gradient gives the desired inequality
        \[
            f(y)-f(x)-\nabla f(x)^\top(y-x) \ge \frac{1}{2L}\|\nabla f(y)-\nabla f(x)\|_2^2.
        \]
    \section{}
        $(a) \Rightarrow (b)$:\\
            If $A \succeq 0$ doesn't hold, there exists $k < 0$ and $v \in R^n$ such that $Av=kv$.\\
            For every $t \in R$, 
            $$f(tv)=\frac{1}{2}k t^2 v^{\top}v-tb^{\top}v$$
            When $t \to \infty$, $f(tv) \to -\infty$, which is a contradiction.\\
            If $b \notin range(A)$.\\
            Since A is symmetric, we can choose n eigenvectors of A to form an orthonormal basis, 
            which means that $R^n = range(A) \oplus Ker(A) $ and $range(A)=Ker(A)^{\perp}$\\
            b can be written as $b_{1}+b_{2}$ where $b_{1} \in range(A)$, $b_{2} \in Ker(A)$ and $b_{1} \ne 0$.\\
            Chosen $x=tb_{2}$, $f(tb_{2})=-tb_{2}^{\top}b_{2}$, which is contracdict to (a).\\
        $(b) \Rightarrow (d)$ and $(b) \Rightarrow (a)$:\\
            There exists unique $y \in R^n$ satisfied $Ay=b$.\\
            For every $x \in R^n$, 
            \begin{align}  \label{eq}
            f(x+y)&= \frac{1}{2}(x+y)^{\top}A(x+y)-b^{\top}(x+y) \nonumber \\
            &=\frac{1}{2}(x^{\top}Ax+y^{\top}Ay+2y^{\top}Ax)-b^{\top}(x+y) \nonumber \\
            &=\frac{1}{2}(x^{\top}Ax+y^{\top}Ay+2b^{\top}y)-b^{\top}(x+y) \nonumber \\
            &=\frac{1}{2}x^{\top}Ax-b^{\top}x+y^{\top}Ay \nonumber \\
            &\geq f(x)
            \end{align}
            This means that $f(x)$ is bounded from below and has a global minimum.\\
        $(d) \Rightarrow (c)$:\\
            It's obvious.\\
        $(c) \Rightarrow (b)$: \\
            At the local minimizer $z$, $\nabla f(z)$ must be 0, which is equivalent to $Az=b$.\\
            For every $t \in R$ and $x \in R^n$, when $||tx||$ is small enough,
            $$ f(z+tx)=f(z)+\frac{1}{2}t^2x^{\top}Ax \geq f(z) $$ 
            This means that $A \succeq 0$.\\
        So we have proven that the above four conditions are equivalent.\\

        \section{}
        We prove that
        \[\lim_{\|x\|\to\infty} f(x)=+\infty.\]

        Assume there exists a level $t_0$ such that the sublevel set
        \[\mathcal{L}(t_0):=\{x\in\mathbb{R}^n:\;f(x)\le t_0\}\]
        is nonempty and contained in a ball $B(0,R)$ for some $R>0$. Pick
        $x_0\in\mathcal{L}(t_0)$ with $f(x_0) \leq t_0$.

        Fix any $x\in\mathbb{R}^n$ with $x\neq x_0$. Because the ray
        $\{\lambda x+(1-\lambda)x_0:\;\lambda\ge0\}$ meets the sphere
        $\{y:\|y\|=R\}$ at a unique point, there is a $\lambda_x\in(0,1]$ such that
        \[y_x:=\lambda_x x+(1-\lambda_x)x_0,\qquad \|y_x\|=R.
        \]
        By $\mathcal{L}(t_0)\subset B(0,R)$ we have $y_x\notin\mathcal{L}(t_0)$,
        hence $f(y_x) > t_0$.

        By convexity of $f$,
        \[f(y_x)\le\lambda_x f(x)+(1-\lambda_x)f(x_0).
        \]
        Rearranging gives
        \[f(x)\ge\frac{f(y_x)-(1-\lambda_x)f(x_0)}{\lambda_x}
            =\frac{f(y_x)-f(x_0)}{\lambda_x}+f(x_0)
        \]
        Since $\|y_x\|=R$ and $y_x$ lies on the segment between $x_0$ and $x$, one has
        $\lambda_x\asymp R/\|x\|$ as $\|x\|\to\infty$, so $\lambda_x\to0^+$ and
        $1/\lambda_x\to\infty$. Because $f(y_x)-f(x_0)>0$, the right-hand side
        tends to $+\infty$ as $\|x\|\to\infty$, proving the claim.
    \section{}
        Consider a convex function $f:R \to R$ and $K=\left[0,1\right]$.\\
        First we prove that $f(x)$ is bounded on $\left[0,1\right]$:\\
        For every $x \in \left(0,1\right)$, 
        $$ f(x) \leq xf(1)+(1-x)f(0) $$
        which means that $f(x) \leq \max(f(1),f(0))$. $f(x)$ is upper bounded on $\left[0,1\right]$. \\
        We can prove that $f(x)$ is upper bounded on any bounded closed interval the same as above.\\
        If there exists a sequence $\left \{x_{n}\right \} \subset \left[0,1\right]$,
        satisfying $f(x_{n}) \to -\infty$ when $n \to \infty$, by the compactness of $\left[0,1\right]$, 
        without losing generality, let $x_{n}\to y \in \left[0,1\right]$.\\
        $2y-x_{n} \in \left[-1,2\right]$, let $M$ be an upper bound of $f(x)$ on $\left[-1,2\right]$,we have:\\
        $$ \frac{1}{2}(f(x_{n})+f(2y-x_{n})) \geq f(y)$$
        $$f(x_{n}) \geq 2f(y)-f(2y-x_{n}) \geq 2f(y)-M$$
        The inequality holds for every positive integer $n$, which is contradict to the hypothesis.\\
        So $f(x)$ is bounded from below.\\
        \\
        By the convexity of $f(x)$, for every $x_{1} < x_{2} < x_{3} < x_{4}$, we have:\\
        $$ \frac{f(x_{1})-f(x_{2})}{x_{1}-x_{2}} \leq  \frac{f(x_{3})-f(x_{4})}{x_{3}-x_{4}} $$
        Let $N$ be an upper bound of $|f(x)|$ on $\left[-1,2\right]$, for every $x,y \in \left[0,1\right]$, we have:\\
        $$ \frac{f(0)-f(-1)}{0-1} \leq \frac{f(x)-f(y)}{x-y} \leq \frac{f(2)-f(1)}{2-1}$$
        $$ -2N \leq \frac{f(x)-f(y)}{x-y} \leq 2N$$
        This shows that $|f(x)-f(y)| \leq 2N|x-y|$, $f$ is Lipschitz continous on $\left[0,1\right]$.
        For $f:R^n \to R$, K is a bounded closed set, and $f$ is bounded on every bounded closed set.\\
        For every $x,y \in R^n$, we can write $y$ as $y-x+x$. Once we fixed $x,y$, we can Consider y-x as a ray starting from x.\\
        So it comes down to the case of n = 1, with the same upper bounds in all directions, which is what we want.\\


    \section{}
        Since $f(x)$ is differentiable, $\nabla f(x^{*}) = 0$.\\
        First we prove:\\
        $$ [\nabla f(x)-\nabla f(y)]^{\top}(x-y) \geq \frac{1}{L}\|\nabla f(x)-\nabla f(y)\|^{2} $$:
        By the conclusion of Problem 1, we have:
        $$f(x)-f(y)-[\nabla f(x)]^{\top}(x-y) \leq-\frac{1}{2 L}\|\nabla f(x)-\nabla f(y)\|_{2}^{2} $$
        and $ f(y)-f(x)-[\nabla f(y)]^{\top}(y-x) \leq-\frac{1}{2 L}\|\nabla f(x)-\nabla f(y)\|_{2}^{2}$ \\
        So we have:
        $$ [\nabla f(x)-\nabla f(y)]^{\top}(x-y) \geq \frac{1}{L}\|\nabla f(x)-\nabla f(y)\|^{2} $$
        Substituting $ x^{*} $ into $f(y)$, we can get
        $$ \nabla f(x)^{\top}(x-y) \geq \frac{1}{L}\|\nabla f(x)\|^{2} $$
        Let $g(t)=\|x-t\nabla f(x)-x^{*} \|^{2}$, then
        $$ g(t)=\|x-x^{*}\|^{2}+t^{2}\|\nabla f(x) \|^{2}-2t(x-x^{*})^{\top}\nabla f(x) $$
        When $0 \leq t \leq \frac{L}{2}$,
        $$ g(t) \leq \|x-x^{*}\|^{2}+t^{2}\|\nabla f(x) \|^{2}-\frac{2}{L}t\|\nabla f(x) \|^{2} \leq \|x-x^{*}\|^{2}$$
    \section{}
        Suppose that convex function $f:R^n \to R$ is differentiable.\\
        When n=1, such a convex function which is differentiable but not continously differentiable does not exist.\\
        Prove:\\
        By the convexity of $f(x)$, for every $x_{1} < x_{2} < x_{3} < x_{4}$, we have:\\
        $$ \frac{f(x_{1})-f(x_{2})}{x_{1}-x_{2}} \leq  \frac{f(x_{3})-f(x_{4})}{x_{3}-x_{4}} $$
        So $f'(x_{1}) \leq f'(x_{3})$ for every $x_{1} < x_{3}$. In other words, $f'(x)$ is monotone increasing.\\
        Moreover, $f'(x)$ has the intermediate value property, which means that $f'(x)$ is contionous.\\
    \section{}
    Let $f(x,y)=(y-x^{2})(y-2x^{2})$. Then $f$ is twice continuously differentiable.
    In a neighborhood of $(0,0)$, for every $k\in\mathbb{R}$,
    \[
    f(x,kx)=x^{2}(k-x)(k-2x).
    \]
    For each fixed $k$ there exists $\delta>0$ such that $0<|x|<\delta$ implies $f(x,kx)>0$, while $f(0,0)=0$.\\
    Thus, along any straight line through $(0,0)$, the point $(0,0)$ is a local minimizer of $f$.
    However,
    \[
    f\!\left(x,\tfrac{3}{2}x^{2}\right)=-\tfrac{1}{4}x^{4}<0\qquad(x\neq0).
    \]
    Therefore $f(x,y)=(y-x^{2})(y-2x^{2})$ provides a counterexample.
    \section{}
    Let $f(x,y)=x^2+y^2+y^3$. Then $f(0,0)=0$.\\
    In a neighborhood of $(0,0)$, for $0<|x|<1$ and $0<|y|<1$ we have $x^2\ge 0$ and $y^2+y^3=y^2(1+y)>0$, so $f(x,y)>0$ for $(x,y)\neq(0,0)$. Therefore $(0,0)$ is a local minimizer.
    The gradient is
    
    \[
    \nabla f(x,y) = \begin{pmatrix}2x \\[4pt] 2y + 3y^2 \end{pmatrix},
    \]
    \\
    which vanishes only at $x=0$ and $y=0$.
    On the other hand, along the line $x=0$ we have $f(0,y)=y^2+y^3$, and as $y\to -\infty$ one has $f(0,y)\to -\infty$. Hence $(0,0)$ is not a global minimizer.
    Thus $f(x,y)=x^2+y^2+y^3$ serves as a counterexample.
    \section{}
        We only prove the conclusion for the case $X_i \sim B(1,p)$, $\forall i=1,\cdots,n $.\\
        $$Z=\sum_{i=1}^{n}X_i \sim B(n,p)$$
        $$ \mathbb{P}(Z < tn)=\mathbb{P}\left(e^{-\lambda \sum_{k=1}^{n} X_{k}} \geq e^{-\lambda t n}\right) \leq \frac{\mathbb{E}\left[e^{-\lambda \sum_{k=1}^{n} X_{k}}\right]}{e^{-\lambda t n}}  $$
        $$ \mathbb{E}\left[e^{-\lambda Z}\right]=\prod_{k=1}^{n} \mathbb{E}\left[e^{-\lambda X_{k}}\right]=\left[p e^{-\lambda}+(1-p)\right]^{n} $$
        $$\mathbb{P}\left(Z \leq t n\right) \leq e^{\lambda t n}\left[p e^{-\lambda}+(1-p)\right]^{n}=\left[e^{\lambda t}\left(p e^{-\lambda}+(1-p)\right)\right]^{n}$$
        Let $f(\lambda)=e^{\lambda t}\left(p e^{-\lambda}+(1-p)\right)$, then\\
        Let $f'(\lambda)=0$,
        $$f^{\prime}(\lambda)=t e^{\lambda t}\left(1-p+p e^{-\lambda}\right)+e^{\lambda t}\left(-p e^{-\lambda}\right)=e^{\lambda t}\left[t\left(1-p+p e^{-\lambda}\right)-p e^{-\lambda}\right]=0 .$$
        we have $e^{-\lambda_0}=\frac{t(1-p)}{p(1-t)}$
        $$ f(\lambda_0)=\left(\frac{p}{t}\right)^{t}\left(\frac{1-p}{1-t}\right)^{1-t}\leq \exp \left[-\frac{(p-t)^{2}}{2 p} \right] $$
        So \\
        $$ \mathbb{P}\left(\sum_{k=1}^{n} X_{k} \leq t n\right) \leq[f(\lambda)]^{n} \leq \exp \left[-\frac{(p-t)^{2}}{2 p} n \right] $$
        \\
    \section{}
        Let $P(A)$ be the set of all eigenvalues of $A$.
        $(a)$Absolute homogeneity(satisfied):\\
        $\rho(A)=\max_{k \in P(A)}|k| $. \\
        Since $k \in P(A) \Leftrightarrow \alpha k \in P(\alpha A)$  we have:\\
        $$|k_0|=\max_{k \in P(A)}|k| \Leftrightarrow |\alpha k_0|=\max_{l \in P(\alpha A)}|l| $$
        So $\rho(\alpha A)=|\alpha| \rho(A)$. \\
        \\
        $(b)$Triangle inequality(unsatisfied):\\
        Let:
        \[
        A = \left[
        \begin{array}{cc}  
            0 & 2  \\
            0 & 0  \\
        \end{array}
        \right]
        \]
        \[
        B = \left[
        \begin{array}{cc}  
            0 & 0  \\
            2 & 0  \\
        \end{array}
        \right]
        \]
        All eigenvalues of $A,B$ are 0, but $A+B$ is nonsingular.\\
        So $\rho (A+B) > \rho(A)+\rho(B)$.\\
        \\
        $(c)$positive definitness(unsatisfied):\\
        Choose $A$ from (b), $\rho (A) = 0$ but $A \ne 0$.\\
        \\
        $(d)$consistency(unsatisfied):\\
        Consider $A,B$ mentioned in $(b)$, $\rho (A)=\rho (B)=0 $, but   \\
        \[
        AB = \left[
        \begin{array}{cc}  
            4 & 0  \\
            0 & 0  \\
        \end{array}
        \right]
        \]
        $\rho (AB)=4 > \rho(A) \rho(B)$, which is a counterexample.\\
        \\
        
    \section{}
        $(a)$:\\
        This is equivalent to proving the following proposition:\\
        For all $x,y \geq 0$, $(x+y)^p \leq x^p+y^p$\\
        Let $g(x)=(x+y)^p-x^p$.\\
        $g'(x)=p(\frac{1}{(x+y)^{1-p}}-\frac{1}{x^{1-p}})<0$ when $x>0$.\\
        So $ (x+y)^p=g(x) \geq g(0)=y^p$.\\
        \\
        $(b)$:\\
        We want to prove:\\
        $$ \|x\|_{p}^p +\|y\|_{p}^p \leq 2^{1-p}(\|x\|_{p}+\|y\|_{p})^p$$
        Let $a=\frac{\|x\|_{p}}{\|x\|_{p}+\|y\|_{p}}$, what we want to prove is:
        $$ a^p+(1-a)^p \leq 2^{1-p} $$
        Let $h(a)=a^p+(1-a)^p$ for $a\in [0,1]$, then we have:\\
        $h'(a)=p(\frac{1}{a^{1-p}}-\frac{1}{(1-a)^{1-p}})$ and 
        $h''(a)=p(p-1)(a^{p-2}+(1-a)^{p-2})<0$.\\
        Since $h'(a)$ has a unique zero $a=\frac{1}{2}$, we have
        $\max_{[0,1]}h(a)=h(\frac{1}{2})=2^{1-p}$\\
        So that $h(a)\leq 2^{1-p}$. Then:\\
        $$ \|x+y\|_{p}^p \leq \|x\|_{p}^p +\|y\|_{p}^p \leq 2^{1-p}(\|x\|_{p}+\|y\|_{p})^p $$
        $$ \|x+y\|_{p}\leq 2^{\frac{1}{p}-1}(\|x\|_{p}+\|y\|_{p}) $$
        \\
        $(c)$:\\
        \\
        $(d)$:\\
        For every $x \in R^n$ and $0<p<q<\infty$, when $x \ne 0$\\
        $$\|x\|_{p} \geq \|x\|_{q} \Leftrightarrow 1 \geq \frac{\|x\|_{q}}{\|x\|_{p}} $$
        Let $y=\frac{x}{\|x\|_{p}}$, then $\|y\|_{q}=\frac{\|x\|_{q}}{\|x\|_{p}}$\\ 
        $$1=\sum_{i=1}^{n}|y_{i}|^p \geq \sum_{i=1}^{n}|y_{i}|^q $$
        So that $\|y\|_{q} \leq 1$, $1 \geq \frac{\|x\|_{q}}{\|x\|_{p}}$\\
        \\
        $(e)$:\\
        Let $f(p)=log\|x\|_{p}=\frac{1}{p}log(\sum_{i=1}^{n}|x_i|^p)$ and $S(p)=\sum_{i=1}^{n}|x_i|^p$.\\
        $f(p)$ is convex if and only if $f''(p) \geq 0$.\\
        $$f^{\prime}(p)=\frac{d}{d p}\left[\frac{1}{p} \log S(p)\right]=-\frac{\log S(p)}{p^{2}}+\frac{S^{\prime}(p)}{p S(p)}$$
        $$ f^{\prime \prime}(p)=\frac{2 \log S(p)}{p^{3}}-\frac{2 S^{\prime}(p)}{p^{2} S(p)}+\frac{S^{\prime \prime}(p) S(p)-\left[S^{\prime}(p)\right]^{2}}{p[S(p)]^{2}} $$
        $$ S^{\prime \prime}(p) S(p)-\left[S^{\prime}(p)\right]^{2}=\left(\sum_{i=1}^{n}|x_i|^p (ln|x_i|)^2\right)\left(\sum_{i=1}^{n}|x_i|^p\right)-\left(\sum_{i=1}^{n}\left|x_{i}\right|^{p} \ln \left|x_{i}\right|\right)^{2}\geq 0 $$
        $$ \frac{2 \log S(p)}{p^{3}}-\frac{2 S^{\prime}(p)}{p^{2} S(p)}=\frac{2}{p^{3}}\left(\log S(p)-\frac{p S^{\prime}(p)}{S(p)}\right) $$
        Let $g(p)=\log S(p)-\frac{p S^{\prime}(p)}{S(p)}$
        $$g(p)=\frac{\sum_{i=1}^{n}(|x_i|^plog(\sum_{j=1}^{n}|x_j|^p))-\sum_{i=1}^{n}|x_i|^p pln|x_i|}{\sum_{i=1}^{n}|x_i|^p}\geq 0$$
        So $f''(p)\geq 0$ for every $p>0$.\\
        \\
        $(f)$:\\
        \\
    \section{}
        For any vector $x \in \mathbb{R}^n$, denote 
        \[
        S_x=\{x+d:\|d\| \le 1  \}
        \]
        We know that in a finite-dimensional linear space, the unit sphere
        \[
        \mathbb{S}=\{y \in \mathbb{R}^n: \|y\|=1\}
        \]
        \\
        is compact. Additionally, the mapping
        \[
        y \mapsto \|Ay\|
        \]
        \\
        is continuous. Therefore, by the extreme value theorem, there exists some $x_0$ with $\|x_0\|=1$ such that
        \[
        \|Ax_0\|=\sup_{\|y\|=1}\|Ay\|=\|A\|
        \]
        Naturally, we expect the larger of $\|Ax-Ax_0\|$ and $\|Ax+Ax_0\|$ to be at least $\|A\|$, that leads us 
        to investigate the relationship between $\|Ax-Ax_0\|$, $\|Ax+Ax_0\|$ and $\|Ax_0\|$.\\
        In fact, by the triangle inequality, we have
        \[
        \|Ax-Ax_0\|+\|Ax+Ax_0\| \ge 2\|Ax_0\|=2\|A\|
        \]
        \\
        This shows that at least one of $\|Ax-Ax_0\|$ and $\|Ax+Ax_0\|$ is no less than $\|A\|$.\\
        In other words, we have
        \[
        \max_{\|d\| \le 1 }\|A(x+d)\|   \ge \max(\|Ax-Ax_0\|,\|Ax+Ax_0\|) \ge \|A\|
        \]
         \\
    \section{}
        $(a)$:\\
        If $\lambda \ne 0$ is an eigenvalue of $AB$ and $ABv=\lambda v$, we have\\
        $$BA(Bv)=B(\lambda v)=\lambda Bv $$
        So $\lambda$ is an eigenvalue of $BA$ and $Bv$ is an eigenvector of $BA$.\\
        Similarly, if $\lambda \ne 0$ is an eigenvalue of $AB$, 
        $\lambda \ne 0$ is also an eigenvalue of $BA$.\\
        \\
        $(b)$:\\
        We are going to prove that, for every $x\ne 0$:\\
        $$ dim(Ker(xI_m-AB))=dim(Ker(xI_n-BA)) $$
        Set $f(v)=Bv$ for every $x\in Ker(xI_m-AB)$.\\
        It's obvious that $Bv\in Ker(xI_n-BA)$.\\
        We can similarly define $g:Ker(xI_n-BA) \to Ker(xI_m-AB)$, $g(w)=Aw$.\\
        Consider $f\circ g:Ker(xI_n-BA) \to Ker(xI_n-BA)$: 
        for every $w\in Ker(xI_n-BA)$, 
        $$ f\circ g(w)=BAw=xw $$
        So $f\circ g$ is injective, which means that $g$ is injective.\\
        Similarly, $f$ is also injective. So we have:\\
        $$ dim(Ker(xI_m-AB))=dim(Ker(xI_n-BA)) $$
        \\
        $(c)$:\\
        $$det(xI_m-AB)=x^{m-n}det(xI_n-BA)$$
        If $\lambda\ne 0$ is an eigenvalue of $AB$ and $BA$, 
        and if the algebraic multiplicity of $\lambda$ respect to $AB$ is k, then
        $$ det(xI_m-AB)=(x-\lambda)^kp(x) $$
        where $l.c.d.(p(x),x-\lambda)=1$.\\
        Since $l.c.d.(x,x-\lambda)=1$, 
        $$ det(xI_n-BA)=(x-\lambda)^kq(x) $$
        where $l.c.d.(q(x),x-\lambda)=1$. So the conclusion holds.\\
        \\
    \section{}
        For matrix $A\in \mathbb{C}^{n \times n}$, there exists a nonsingular matrix $M \in \mathbb{C}^{n \times n}$, 
        such that $M^{-1}JM=A $, where $J$ is the Jordan normal form of $A$.\\
        For every $p\in \mathbb{C}[x]$, since 
        $p(A)=p(M^{-1}JM)=M^{-1}p(J)M$ and $det(xI_n-M^{-1}JM)=det(xI_n-J)$,\ \
        we only need to prove that the conclusion holds for every Jordan block $J_{\lambda}$.\\
        Assume that $J_{\lambda}$ has a dimension $k$.\\
        $(a)$:\\
        $p(J_{\lambda})$ is an upper triangular matrix, whose elements on the diagonal are all $p(\lambda)$.\\
        If the characteristic polynomial of $J_{\lambda}$ is:\\
        $$det(xI_k-J_{\lambda})=(x-\lambda)^k $$
        then the characteristic polynomial of $p(J_{\lambda})$ is:\\
        $$det(xI_k-p(J_{\lambda}))=(x-p(\lambda))^k $$
        So that $\lambda$  is an eigenvalue of $J_{\lambda}$ if and only if $p(\lambda)$ is an eigenvalue of $p(J_{\lambda})$.\\
        $(b)$:\\
        The same as above, it's obvious that if the multiplicity of $\lambda$ is $k$, 
        then the multiplicity $p(\lambda)$ is also $k$.
        \\
    \section{}
        We compute the eigenvalues of matrix A by calculating its characteristic polynomial.
        \[
        \lambda I_n-A= 
        \begin{pmatrix}
        \lambda-1 &-x     &\cdots & -x\\
        -x        &\ddots &\ddots& \vdots \\
        \vdots    &\ddots &\ddots & -x  \\
        -x        &\cdots &-x     &\lambda-1
        \end{pmatrix}
        \]
        \[
        det(\lambda I_n-A)= det
        \begin{pmatrix}
        \lambda-1 &-x     &\cdots & -x\\
        -x        &\ddots &\ddots& \vdots \\
        \vdots    &\ddots &\ddots & -x  \\
        -x        &\cdots &-x     &\lambda-1
        \end{pmatrix}
        \]
        We add the last n-1 columns of this determinant to the first column. 
        It is easy to see that all elements in the first column become equal, 
        thus we can perform the following transformation:
        \[
        det(\lambda I_n-A)
        =(\lambda-1-(n-1)x)\cdot det 
        \begin{pmatrix}
        1         &-x       &-x       &\cdots & -x\\
        1         &\lambda-1&-x       &\cdots & -x\\
        \vdots    &-x       &\ddots   &\ddots & \vdots\\
        \vdots    &\vdots   &\ddots   &\ddots &-x\\
        1         &-x       &\cdots   &-x     &\lambda-1
        \end{pmatrix}
        \]
        In order to make the determinant sparser for easier computation, 
        we multiply the first row by x and add it to the last n-1 rows, 
        thereby eliminating all occurrences of -x:
        \[
        det(\lambda I_n-A)= (\lambda-1-(n-1)x)\cdot det
        \begin{pmatrix}
        1         &0          &0      &\cdots & 0\\
        1         &\lambda-1+x&0      &\cdots & 0\\
        1         &0          &\lambda-1+x&\ddots & \vdots\\
        \vdots    &\vdots     &\ddots     &\ddots &0\\
        1         &0        &\cdots   &0      &\lambda-1+x
        \end{pmatrix}
        \]
        $$det(\lambda I_n-A)=(\lambda-1-(n-1)x)(\lambda-1+x)^{n-1}$$
        $(a)$:\\
        The eigenvalues of $A$ are $1-x$ (order $n-1$) and $1+(n-1)x$ (order $1$).\\
        \\
        $(b)$:\\
        The symmetric matrix $A$ is positive definite if and only if all the eigenvalues of $A$ are positive.
        That is to say, $1-x>0$ and $1+(n-1)x>0$.\\
        So we have
        $$ -\frac{1}{n-1} < x < 1 $$
        \\
    \section{}
        $ $
        \\
    \section{}
        $(a)$:\\
        If $0$ is an eigenvalue of $J$, $J$ is singular, which means that $A$ is singular 
        and $0$ is an eigenvalue of $A$.\\
        In the following, we only consider nonzero eigenvalues of $J$.\\
        \[
        det(xI_{2n}-J)=det
        \begin{pmatrix}
        xI_n & -A\\
        -A^{H} &xI_n
        \end{pmatrix}
        \]
        \[
        det
        \begin{pmatrix}
        xI_n & -A\\
        -A^{H} &xI_n
        \end{pmatrix}=det
        \begin{pmatrix}
        xI_n & -A\\
        -A^{H} &xI_n
        \end{pmatrix}
        \begin{pmatrix}
        I_n & 0\\
        \frac{1}{x} A^{H} &I_n
        \end{pmatrix}=
        \begin{pmatrix}
        xI_n-\frac{1}{x}A & -A\\
        0 &xI_n
        \end{pmatrix}
        \]
        $$ det(xI_{2n}-J)=x^n det(xI_n-\frac{1}{x}AA^{H})=det(x^2I_n-AA^{H}) $$
        So if the eigenvalues of  $A^{H} A$  are  $\sigma_{1}, \ldots, \sigma_{n}$ , multiplicity included, 
        the eigenvalues of  J  are  $\sqrt{\sigma_{1}},-\sqrt{\sigma_{1}}, \ldots, \sqrt{\sigma_{n}},-\sqrt{\sigma_{n}}$ , multiplicity included.
        \\
        $(b)$:\\
        Denote 
        \[
        U=
        \begin{pmatrix}
        U_1 & U_2\\
        V_1 & V_2
        \end{pmatrix}
        \]
        $U$ is an unitary matrix, so:\\
        \[
        \begin{pmatrix}
        U_1 & U_2\\
        V_1 & V_2
        \end{pmatrix}
        \begin{pmatrix}
        U_1 & U_2\\
        V_1 & V_2
        \end{pmatrix} ^{H}=
        \begin{pmatrix}
        U_1 & U_2\\
        V_1 & V_2
        \end{pmatrix}
        \begin{pmatrix}
        U_1^{H} & V_1^{H}\\
        U_2^{H} & V_2^{H}
        \end{pmatrix}=
        \begin{pmatrix}
        I_n & 0\\
        0   & I_n
        \end{pmatrix}
        \]
        So we have 
        $$ U_1 V_1^{H}+U_2 V_2^{H}=0 \Rightarrow U_1 V_1^{H}=-U_2 V_2^{H}$$
        \[
        U 
        \begin{pmatrix}
        \Sigma & 0\\
        0      & \Sigma     
        \end{pmatrix} U^{H}=
        \begin{pmatrix}
        U_1\Sigma U_1^{H}-U_2\Sigma U_2^{H} & U_1\Sigma V_1^{H}-U_2\Sigma V_2^{H}\\
        V_1\Sigma U_1^{H}-V_2\Sigma U_2^{H} & V_1\Sigma V_1^{H}-V_2\Sigma V_2^{H}   
        \end{pmatrix}
        \]
        \[
        U 
        \begin{pmatrix}
        \Sigma & 0\\
        0      & \Sigma     
        \end{pmatrix} U^{H}=J=
        \begin{pmatrix}
        0     & A\\
        A^{H} & 0    
        \end{pmatrix}
        \]
        So that 
        $$ A=U_1\Sigma V_1^{H}-U_2\Sigma V_2^{H}=2U_1\Sigma V_1^{H}=-2U_2\Sigma V_2^{H} $$
        \\
    \section{}
        $(a)$:\\
        
        $$ w_{k}=v_{k+1}-v_{k}=\sqrt{\frac{k+2}{2(k+1)}}e_{k+1}+(\frac{1}{\sqrt{2k(2k+1)}}-\sqrt{\frac{k+1}{2k}})e_k $$
    \section{}
        Set $T:R^{m\times n}\to R^{m\times n}$, $TX=AX-XB$. $T$ is a linear map.\\
        $TX=C$ has a unique solution for all $C\in R^{m\times n}$ if and only if $T$ is injective.\\
        "Only if" part: $AX-XB=C$ has a unique solution.\\
        If $A,B$ share the same eigenvalue $k$, there exists $u\in R^{m}$ and $v\in R^n $, 
        such that $Au=ku$ and $Bv=kv$.\\
        Let $X=uv^{\top}$. Then 
        $$ AX-XB=Auv^{\top}-uv^{\top}B=kuv^{\top}-kuv^{\top}=0 $$
        $TX=0$ has a nontrivial solution, so that $TX=C$ has no solution or at least 2 solutions, \\
        which is a contradiction.\\
        \\
        "If" part: $A,B$ do not share any eigenvalue.\\
        If $TX=C$ has at least 2 solutions for some $C\in R^{m\times n}$, 
        which means that $TX=0$($AX=XB$) has nontrivial solution.\\
        Let $M$ be the unique solution of $AX=XB$.\\
        Let $p(x)$ and $q(x)$ be the characteristic polynomial of $A$and $B$.\\
        Then we have $0=p(A)X=Xp(B)$ and $q(A)X=Xq(B)=0$.\\
        Since $X$ is nontrivial, there exists a $w\in R^m$ such that $w^{\top}X\ne 0 $, then\\
        $$ w^{\top}Xp(B)=0 $$
        $p(B)$ is singular, so that $0$ is an eigenvalue of $p(B)$.\\
        Since all the eigenvalues of $p(B)$ are $p(\lambda)$ where $\lambda$ are the eigenvalues of $B$.\\
        So $p(\lambda_0)=0$ holds for some eigenvalues $\lambda_0 $of $B$, \\
        which means that $\lambda_0$ is an eigenvalue of $A$, which is a contradiction.\\
        So $T$ is injective, and  \\
        $TX=C$ has a unique solution for all $C\in R^{m\times n}$.\\
        \\
    \section{}
        Let $g(x)$ be the $c.d.f$ of $X$, that is, $g(-\infty)=0$ and  $g(+\infty)=1$.\\
        For every finite partition of interval $[-\infty,+\infty]$, take $\{x_k\}_{k=0}^{n}$ be the partition points, 
        where \\
        $$-\infty=x_0<x_1<\dots <x_n=+\infty$$ 
        By the convexity of $f(x)$, we have
        $$ f(\sum_{i=0}^{n-1}(g(x_{i+1})-g(x_i))x_i) \leq \sum_{i=0}^{n-1}f(x_i)((g(x_{i+1})-g(x_i))) $$
        Let $n\to \infty$ and $ \max_{1\leq i \leq n-2}(x_{i+1}-x_{i}) \to 0$, we have\\
        $$ f(\int_{-\infty}^{+\infty}x\mathrm{d}g(x)) \leq \int_{-\infty}^{+\infty}f(x)\mathrm{d}g(x)$$
        $$ f(\mathbb{E} (X)) \leq \mathbb{E}(f(X)) $$
        \\


    \section{}
        $$\int_{0}^{1}f(x)\mathrm{d}x-f(\frac{1}{2})= \int_{0}^{1}f(x)-f(\frac{1}{2})\mathrm{d}x$$
        $$ \int_{0}^{1}f(x)-f(\frac{1}{2})\mathrm{d}x=\int_{\frac{1}{2}}^{1}f(x)-f(\frac{1}{2})\mathrm{d}x+\int_{0}^{\frac{1}{2}}f(x)-f(\frac{1}{2})\mathrm{d}x $$
        $$\int_{0}^{1}f(x)-f(\frac{1}{2})\mathrm{d}x=\int_{\frac{1}{2}}^{1}f(x)-f(\frac{1}{2})\mathrm{d}x-\int_{\frac{1}{2}}^{1}f(\frac{1}{2})-f(1-x)\mathrm{d}x $$
        Since $f(x)$ is convex, $f(x)-f(\frac{1}{2})\geq f(\frac{1}{2})-f(1-x)$ holds for $x\in[\frac{1}{2},1]$. \\
        So $ \int_{0}^{1}f(x)\mathrm{d}x \geq f(\frac{1}{2})$.\\
        $$\frac{1}{2}(f(0)+f(1))-\int_{0}^{1}f(x)\mathrm{d}x= \int_{\frac{1}{2}}^{1}f(1)-f(x)\mathrm{d}x+\int_{0}^{\frac{1}{2}}f(0)-f(x)\mathrm{d}x $$
        $$ \frac{1}{2}(f(0)+f(1))-\int_{0}^{1}f(x)\mathrm{d}x= \int_{\frac{1}{2}}^{1}f(1)-f(x)\mathrm{d}x-\int_{\frac{1}{2}}^{1}f(1-x)-f(0)\mathrm{d}x$$
        When $x\in[\frac{1}{2},1]$, $x\geq 1-x$. So $ f(1)-f(x) \geq f(1-x)-f(0)$, which means that\\
        $$\int_{\frac{1}{2}}^{1}f(1)-f(x)\mathrm{d}x-\int_{\frac{1}{2}}^{1}f(1-x)-f(0)\mathrm{d}x \geq 0 $$
        So $\frac{1}{2}(f(0)+f(1))\geq \int_{0}^{1}f(x)\mathrm{d}x  $.\\
        \\
    \section{}
        Assume that $X$, $Y$ are $i.i.d.$ random variables. Then $ \mathbb{E}(f(Y)g(X))=\mathbb{E}(g(X))\mathbb{E}(f(Y)) $\\
        $$(f(X)-f(Y))(g(X)-g(Y))\geq 0$$
        $$ \mathbb{E}(f(X)-f(Y))(g(X)-g(Y))\geq 0 $$
        $$ \mathbb{E}(f(X)-f(Y))(g(X)-g(Y))=\mathbb{E}(f(X)g(X))+\mathbb{E}(f(Y)g(Y))-\mathbb{E}(f(X)g(Y))-\mathbb{E}(f(Y)g(X)) $$
        Since $\mathbb{E}(f(X)g(X))=\mathbb{E}(f(Y)g(Y))$ and $\mathbb{E}(f(Y)g(X))=\mathbb{E}(f(Y)g(X))$,\\
        $$ 2(\mathbb{E}(f(X)g(X))-\mathbb{E}(f(Y)g(X)))\geq 0 $$
        $$ \mathbb{E}(f(X)g(X))-\mathbb{E}(g(X))\mathbb{E}(f(Y))\geq 0 $$
    \section{}
        We have rearrangement inequality:\\
        For any rearrangement $\sigma$ of the sequence $0, 1, ..., n$, we have:\\
        $$ \sum_{k=0}^{n} a_{k} b_{n-k} \leq \sum_{k=0}^{n} a_{k} b_{\sigma(k)} \leq \sum_{k=0}^{n} a_{k} b_{k}$$
        Consider a rearrangement $\tau$, where 
        $$\tau(k)\equiv k+1 \pmod{n+1}$$ 
        then 
        $$\tau ^m(k)\equiv k+m \pmod{n+1}$$
        We first analyze the expression
        $$ \left(\sum_{k=0}^{n} a_{k}\right)\left(\sum_{k=0}^{n} b_{k}\right) $$
        Let us consider the matrix $A$, whose $(i,j)$-th entry is $a_{i}b_{j}$ for $0\leq i,j \leq n$
        \[
        A=
        \begin{pmatrix}
        a_{0}b_{0} & a_{0}b_{1} & \cdots & a_{0}b_{n} \\
        a_{1}b_{0} & a_{1}b_{1} & \cdots & a_{1}b_{n} \\
        \vdots     & \vdots     & \ddots & \vdots     \\
        a_{n}b_{0} & a_{n}b_{1} & \cdots & a_{n}b_{n} \\
        \end{pmatrix}
        \]\\
        Then each term in the above expression corresponds to an entry in this matrix. So we are going to 
        compute the sum of all elements in the matrix.
        Let us consider the diagonals of this matrix that are parallel to the main diagonal. \\
        We denote the main digonal as the $0$-th diagonal. \\
        The $i$-th diagonal is defined as the diagonal obtained by shifting the main diagonal $i$ places to the right
        (if $i$ is negative, then we shift to the left). \\
        There are $n+1$ such diagonals, and the sum of the elements on the $i$-th diagonal and $i-n-1$-th digonal is
        $$ \sum_{j=0}^{n} a_{j}b_{\tau^{i}(j)} $$
        for $0\leq i \leq n$.\\
        Then the sum of all elements in matrix $A$ is
        $$ \left(\sum_{k=0}^{n} a_{k}\right)\left(\sum_{k=0}^{n} b_{k}\right)
        = \sum_{i=0}^{n} \sum_{j=0}^{n}a_{j}b_{\tau^{i}(j)} $$
        So by rearrangement inequality, we have:\\
        $$ \sum_{k=0}^{n} a_{k} b_{n-k} \leq \frac{1}{n+1}\left(\sum_{k=0}^{n} a_{k}\right)\left(\sum_{k=0}^{n} b_{k}\right) \leq \sum_{k=0}^{n} a_{k} b_{k} $$
    \\
    \section{}
        Let $x_n=\sum_{k=1}^{n}\frac{1}{k}$, then $x_{k+1}-x_k=\frac{1}{k+1}$.\\
        For every $\varepsilon>0 $, there exists $n_0$ such that $\frac{1}{n_0+1}<\varepsilon$.\\
        $$ \sum_{|x_{k+1}-x_k|>\varepsilon}|x_{k+1}-x_k| < \sum_{k=1}^{n_0+1}\frac{1}{n_0+1}<\infty $$
        But $\lim_{n\to \infty}x_n=+\infty$.\\
        Conversely, if $\{x_n\}\subset R$ converges, for every $\varepsilon>0 $, there exists $N$ such that \\
        if $n,m>N$, $|x_m-x_n|<\varepsilon$. So we have\\
        $$ \sum_{|x_{k+1}-x_k|>\varepsilon}|x_{k+1}-x_k|<\sum_{k=1}^{N+1}|x_{k+1}-x_k|<+\infty $$.\\
        \\
     \section{}
        When (a) holds:\\
        Let $\{a_k\}$ and $\{b_k\}$ be nonnegative sequences. If $a_k\le b_k$ for all $k$, then
        \[
            \sum_{k=0}^{\infty} a_k \le \sum_{k=0}^{\infty} b_k
        \]
        Moreover, the inequality
        \[
            \sum_{k=0}^{\infty} b_k \le 2a_0 + 4\sum_{k=0}^{\infty} b_k
        \]
        is exactly true since $a_0$ is nonnegative.\\
        \\
        When (b) holds:\\
        By induction, we can have 
        \[
        a_k=\dfrac{a_0}{2^k}
        \]
        holds for all nonnegative integers k. Then
        \[
            \sum_{k=0}^{\infty} a_k = \frac{a_0}{1-\tfrac{1}{2}} = 2a_0
        \]
        Hence
        \[
            \sum_{k=0}^{\infty} a_k = 2a_0 \le 2a_0 + 4\sum_{k=0}^{\infty} b_k
        \]
        \\
    \section{}
        $(a)$:\\
        Let $g(x)=\|x-Tx\|$.\\
        Since $X$ is a compact set and $g(x)$ is a continous function, there exists $y$ such that:\\
        $$g(y)=\inf_{x\in X}g(x) $$
        If $y \ne Ty$, we have\\
        $$ g(Ty)=\|Ty-T^2 y\|<\|y-Ty\|=g(y) $$
        which is a contradiction. So $y=Ty$. \\
        If $z \in X$ satisfies $z=Tz$, then\\
        $$\|Tz-Ty\|=\|z-y\| \Rightarrow z=y $$
        So $T$ has a unique fixed point.\\
        \\
        $(b)$:\\
        Let $y$ be the fixed point of T,and $h(x)=\|x-y\|$. $h(x)$ is contionous.\\
        If there exists $n_0$ such that $Tx_{n_0}=x_{n_0}$, then for $n\geq n_0$, $x_{n_0}=y$.\\
        Let's make all $x_n \ne x_{n+1}$.\\
        $$h(x_{n+1})=\|x_{n+1}-y\|=\|Tx_n-Ty\|<\|x_n-y\|=h(x_n) $$
        Since $h(x_n)\geq 0$, $h(x_n)$ has the greatest lower bound $M \geq 0$.\\
        There exists a subsequence $\{x_{n_{k}}\}$ such that $h(x_{n_{k}})\to M$.\\
        $\{x_{n_{k}}\}$ has a convergent subsequence. 
        For simplicity, let $\{x_{n_{k}}\}$ be convergent, $x_{n_{k}}\to z\in X$ .\\
        So $\|z-y\|=M$ and $x_{n_{k}+1}=Tx_{n_{k}} \to Tz$. If $z\ne y$, we have:\\
        $$ \|x_{n_{k}+1}-y\| \to \|Tz-y\| < \|z-y\| =M$$
        which is a contradiction. So $x_{n_{k}}\to y$.\\
        Since $\|x_n-y\|$ is decreasing, $\|x_n-y\| \to 0$, which means that $x_n \to y$.\\
        \\
    \section{}
        I think that it has a counterexample $f(x)$ that $f(x)$ has at least 2 fixed points and $|f'(x)|>1$ near the fixed points.\\ 
        \\
    \section{}
        Let $A=f(0)-f(1)$. There exists $y\in [0,1]$ such that $|f'(y)|=\max_{x\in [0,1]}|f'(x)|\geq 2|A|$.\\
        Since $f'(0)=f'(1)=0$, there exists $c\in (0,1)$ such that $f''(c)=0$.\\
        If $|f'(y)|=0$, then $k=0$ and $f''(x)\equiv 0$.\\
        If $|f'(y)|>0$, there exists $a\in (0,y)$ and $b\in (y,1)$, such that\\
        $$ f'(y)-f'(0)=f''(a)y $$
        $$ f'(1)-f'(y)=f''(b)(1-y) $$
        So we have
        $ |f''(a)|=\frac{|f'(y)|}{y} $ and $ |f''(b)|=\frac{|f'(y)|}{1-y} $. Then \\
        $$\max(|f''(a)|,|f''(b)|)\geq \max(\frac{2|A|}{y},\frac{2|A|}{1-y}) \geq 4|A|$$
        Since $|f''(c)|=0$, there exists $\xi \in (0,1)$ such that 
        $$|f''(\xi)|=4|A|=4|f(0)-f(1)|$$
    \section{}
        $(a)$:\\
        Define $f:R\to R$, $f(x)=0$ when $x\leq 0$.\\
        Let $a_n=\sum_{i=1}^{n}\frac{1}{4^n}$, then $a_n \to \frac{1}{3}$.\\
        In $[0,1]$, when $x\in [0,\frac{1}{8}]$, $f(x)=8x$, when $x\in [\frac{1}{8},\frac{1}{4}]$, $f(x)=-8x+2$, $f(x)=0$ when $x\in [\frac{1}{4},1]$.\\
        For every positive integer $n$, when $x\in [n,n+a_n]$, $f(x)=0$, \\
        when $x\in [n+a_n,n+a_n+\frac{1}{2\cdot 4^{n+1}}]$, $f(x)=2\cdot 4^{n+1}(x-n-a_n)$, \\
        when $x\in [n+a_n+\frac{1}{2\cdot 4^{n+1}},n+a_n+\frac{1}{4^{n+1}}] $, $f(x)=2\cdot 4^{n+1}(-x+n+a_n+\frac{1}{4^{n+1}})$,\\
        when $x\in [n+a_n+\frac{1}{4^{n+1}},n+1]$, $f(x)=0$.\\
        Denote $A_n=[n+a_n,n+a_n+\frac{1}{\cdot 4^{n+1}}]$, then $f(x)\ne 0$ if and only if there exists $n_0$ and $x\in A_{n_0}$.\\
        Let $B_n=[a_n,a_n+\frac{1}{\cdot 4^{n+1}}]$. $B_n$ are pairwise disjoint and $\bigcup_{n=1}^{\infty}B_n=[\frac{1}{4},\frac{1}{3}]$.\\
        So for every $x\in R$, there exists unique $n_{x}$ and $ k_{x} $,\\
        such that $ x+k \in A_{n} $ if and only if $k=k_{x}$ and $ n=n_{x} $.\\
        So for every $x\in R$, $\lim_{k\to \infty}f(x+k)=0$.\\
        But $f(n+a_n+\frac{1}{2\cdot 4^{n+1}})=1$, $\lim_{x\to \infty}f(x)\ne 0$.\\
        \\
        $(b)$:\\
        \\
    \section{}
        $(a)$:\\
        Let $g(x)=\int_{0}^{x}|f(t)|^2\mathrm{d}t$. Then $0\leq g(x)\leq f(x)$ and $g(0)=0$.\\
        If $\min_{x\in [0,1]}f(x)=0$, $\min_{x\in [0,1]}f(x)<2$.\\
        If $\min_{x\in [0,1]}f(x)>0$, 
        For every $x\in (0,1]$, we have:\\
        $$ \int_{x}^{1} \frac{g'(t)}{g^2(t)}\mathrm{d}t \geq \int_{x}^{1}\mathrm{d}t=1-x$$
        $$ \int_{x}^{1} \frac{g'(t)}{g^2(t)}\mathrm{d}t = \frac{1}{g(x)}-\frac{1}{g(1)} $$
        $$g(x) \leq \frac{1}{1-x+\frac{1}{g(1)}} < \frac{1}{1-x}$$
        $$ x(\min_{y\in [0,1]}f(y))^2 \leq \int_{0}^{x}|f(t)|^2\mathrm{d}t =g(x)<\frac{1}{1-x} $$
        $$ (\min_{y\in [0,1]}f(y))^2 < \frac{1}{x(1-x)} $$
        choose $x=\frac{1}{2}$, $(\min_{y\in [0,1]}f(y))^2 < 4$, which means that
        $$\min_{y\in [0,1]}f(y) < 2$$
        $(b)$:\\
        By the proof of $(a)$, there does not exist a function $f$ satisfying:\\
        $\min_{y\in [0,1]}f(y) = 2$ and $\int_{0}^{x}|f(t)|^2\mathrm{d}t \geq f(x)$.\\
        But I'm not sure whether the bound is tight or not.\\
        \\
    \section{}
        If $A$ is singular, 
        $$\min_{\|x\|_2}\|Ax\|_{\infty}=0 \leq \frac{1}{n}\|A\|_{F} $$
        If $A$ is nonsingular:\\
        Denote $A=(a_1,a_2,\cdots,a_n)^{\top}$, then \\
        $$\|A\|_{F}=(\sum_{i=1}^{n}\|a_i\|_2^2)^{\frac{1}{2}}$$
        Since $\sqrt{x}$ is concave funtion,\\
        $$\frac{1}{n}\|A\|_{F}=\frac{1}{n}(\sum_{i=1}^{n}\|a_i\|_2^2)^{\frac{1}{2}}
        \leq (\sum_{i=1}^{n}\frac{\|a_i\|_2^2}{n})^{\frac{1}{2}}$$
        Let $a_k$ satisfied $\|a_k\|_2=\min_{i=1,\cdots,n}\|a_i\|_2$,\\
        We can find $y\in \{\|x\|_2=1\}$ such that $y^{\top}a_i=0$ holds for all $i\ne k$.\\
        Then we have:\\
        $$ \|Ax\|_{\infty}=|y^{\top}a_k|\leq \|a_k\|_2 \leq \frac{1}{n}(\sum_{i=1}^{n}\|a_i\|_2^2)^{\frac{1}{2}}$$
        $$\min_{\|x\|_2=1}\|Ax\|_{\infty}\leq \frac{1}{n}\|A\|_{F}$$
        
        
    \section{}
        If the set $S$ satisfying those conditions exists:\\
        In numerical computation, due to the characteristics of floating-point calculation, 
        we cannot accurately reach 0.\\
        So we need to set $\varepsilon>0$ that is close enough to 0. 
        If a floating number $x$ satisfying $|x|<\varepsilon$, we take $x$ as 0.\\
        Due to the existence of $S$, once we fixed $\varepsilon$, 
        in the sense of floating-point calculations, \\
        the cardinality of a set of unit vectors that are pairwise orthogonal is at least $\exp (cn\varepsilon ^2)$, 
        which may exceed $n$.\\
        So we cannot rely on such a theory in numerical computation.\\
        


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% References
% Include references only if there are citations.
\ifnum\value{cite}>0
    \small
    \bibliography{\bibfile}
    \bibliographystyle{plain}
\fi

%% The end
\end{document}
